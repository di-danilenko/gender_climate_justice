---
title: "stm.R"
author: "D. Danilenko"
date: "2023-08-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("stm")
library("quanteda")
library("stminsights")
library("tidyverse")
library("readxl")
library("dplyr")
library("ggplot2")
library("tidytext")
library("gutenbergr")
library("reshape2")
library("gridExtra")
library("forestplot")
library("wesanderson")
library("tidyr")
```

```{r}
data <- read.csv('data/df_full.csv', header = TRUE) 
data <- data %>%
  select(-X)
data$text <- "text"
```

```{r}
### DATA PROCESSING ###
# building a corpus using quanteda
# combine title, abstract and keywords in a column called text
data$text <- paste(data$title, data$abstract, data$keywords)

# get rid of the copyright information in our corpus
data <- data %>% separate(text, c("text","copyright"), sep = "\\(c\\)\\s*\\d+", extra="merge", remove = TRUE) #removes (c) followed by numbers - this covers most copyright messages
data$text <- gsub("all rights reserved"," ",as.character(data$text))

# create the actual CORPUS 
corp <- corpus(data, text_field = "text")
summary(corp, 3)

# the function automatically assumes all other columns contain document variables
head(docvars(corp)) 

# create TOKENS from this - this basically just means cutting up the text into individual words
toks <- tokens(corp)
#the below is a fairly standard list for pre-processing
toks <- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE) %>% #create tokens w/o punctuation or symbols
  tokens_tolower() %>% #lowercase -- note the pipe means this function doesn't need any input
  tokens_remove(pattern = stopwords("en"), #remove stopwords
                min_nchar = 2) %>% #remove short words. AMR is three letters, so best keep it to 2
  tokens_wordstem(language = "en") #Using snowball stemmer

# create two dfms: one with unigrams and one with bi-grams
dfm_single = dfm(toks)%>%
  dfm_trim(min_termfreq = 100, termfreq_type = "count", max_docfreq=0.95, docfreq_type = "prop")

toks_bigram <- tokens_ngrams(toks, n = 2, skip = 1:2)
dfm_bigram <- dfm(toks_bigram) %>%
  dfm_trim(min_termfreq = 100, termfreq_type = "count", max_docfreq=0.95, docfreq_type = "prop")

# combine
dfm = cbind(dfm_bigram, dfm_single)

# show most-frequent tokens:
topfeatures(dfm, 15) # should include some bi-grams now
```

```{r}
### TOPIC MODELLING ###
storage <- manyTopics(dfm, seed=1608,
                      K=c(75,100,125),
                      runs = 12,
                      prevalence =~first_author_female + last_author_female + majority_female_binary + 
                        s(year) + subfield + X1_gii_quartile + X2_gii_quartile + impact,
                      data = docvars(dfm),
                      max.em.its = 100,
                      control = list(alpha = 0.5, #Lower than default of 50/k to allow documents to have more topics -> makes small topics more likely to show up
                                     eta = 0.1))  #Higher than default of 0.01 to create topics composed of more words -> more clarity for complex topics

```

```{r}
# get the models and save them
model1 <- storage$out[[1]]
model2 <- storage$out[[2]]
model3 <- storage$out[[3]]

saveRDS(model3, file = "170823_3.Rds")

# explore the outputs
# see the topic frequency with the top n keywords:
plot.STM(model1, type="summary", n = 5, xlim=c(0,.12))
plot.STM(model2, type="summary", n = 5, xlim=c(0,.12))
plot.STM(model3, type="summary", n = 5, xlim=c(0,.12))

# print out topics with keywords
topics <- data.frame(labelTopics(model1, c(1:model1$settings$dim$K), n=10)$prob)
topics$vocab <- "text"
topics$vocab <- paste(topics$X1,topics$X2,topics$X3,topics$X4,topics$X5,topics$X6,topics$X7, topics$X8, topics$X9, topics$X10)
topics = subset(topics, select = -c(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10))

topics2 <- data.frame(labelTopics(model2, c(1:model2$settings$dim$K), n=10)$prob)
topics2$vocab <- "text"
topics2$vocab <- paste(topics2$X1,topics2$X2,topics2$X3,topics2$X4,topics2$X5,topics2$X6,topics2$X7, topics2$X8, topics2$X9, topics2$X10)
topics2 = subset(topics2, select = -c(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10))

topics3 <- data.frame(labelTopics(model3, c(1:model3$settings$dim$K), n=10)$prob)
topics3$vocab <- "text"
topics3$vocab <- paste(topics3$X1,topics3$X2,topics3$X3,topics3$X4,topics3$X5,topics3$X6,topics3$X7, topics3$X8, topics3$X9, topics3$X10)
topics3 = subset(topics3, select = -c(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10))

write.csv(topics,"topics1.csv")
write.csv(topics2,"topics2.csv")
write.csv(topics3,"topics3.csv")
```






